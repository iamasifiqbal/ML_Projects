{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Absenteeism Exercise - Logistic Regression_prior to custom_scaler.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamasifiqbal/ML_Projects/blob/main/Absenteeism_Exercise_Logistic_Regression_prior_to_custom_scaler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx4lko7aPU2m"
      },
      "source": [
        "# Creating a logistic regression to predict absenteeism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOfQszoWPU2n"
      },
      "source": [
        "## Import the relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbKtuMO4PU2o"
      },
      "source": [
        "# import the relevant libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57XQYAoxPU2t"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrpBTSEvPU2t"
      },
      "source": [
        "# load the preprocessed CSV data\n",
        "data_preprocessed = pd.read_csv('Absenteeism_preprocessed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjr8MDutPU2x"
      },
      "source": [
        "# eyeball the data\n",
        "data_preprocessed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Z3tkE0PU21"
      },
      "source": [
        "## Create the targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id46Gu4aPU22"
      },
      "source": [
        "# find the median of 'Absenteeism Time in Hours'\n",
        "data_preprocessed['Absenteeism Time in Hours'].median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQqstJxePU25"
      },
      "source": [
        "# create targets for our logistic regression\n",
        "# they have to be categories and we must find a way to say if someone is 'being absent too much' or not\n",
        "# what we've decided to do is to take the median of the dataset as a cut-off line\n",
        "# in this way the dataset will be balanced (there will be roughly equal number of 0s and 1s for the logistic regression)\n",
        "# as balancing is a great problem for ML, this will work great for us\n",
        "# alternatively, if we had more data, we could have found other ways to deal with the issue \n",
        "# for instance, we could have assigned some arbitrary value as a cut-off line, instead of the median\n",
        "\n",
        "# note that what line does is to assign 1 to anyone who has been absent 4 hours or more (more than 3 hours)\n",
        "# that is the equivalent of taking half a day off\n",
        "\n",
        "# initial code from the lecture\n",
        "# targets = np.where(data_preprocessed['Absenteeism Time in Hours'] > 3, 1, 0)\n",
        "\n",
        "# parameterized code\n",
        "targets = np.where(data_preprocessed['Absenteeism Time in Hours'] > \n",
        "                   data_preprocessed['Absenteeism Time in Hours'].median(), 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7UEnNcfIPU28"
      },
      "source": [
        "# eyeball the targets\n",
        "targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHqbXgL_PU3A"
      },
      "source": [
        "# create a Series in the original data frame that will contain the targets for the regression\n",
        "data_preprocessed['Excessive Absenteeism'] = targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noca4xUMPU3E"
      },
      "source": [
        "# check what happened\n",
        "# maybe manually see how the targets were created\n",
        "data_preprocessed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMAoEZzLPU3H"
      },
      "source": [
        "## A comment on the targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgiVmM3APU3I"
      },
      "source": [
        "# check if dataset is balanced (what % of targets are 1s)\n",
        "# targets.sum() will give us the number of 1s that there are\n",
        "# the shape[0] will give us the length of the targets array\n",
        "targets.sum() / targets.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2BRz_gXPU3L"
      },
      "source": [
        "# create a checkpoint by dropping the unnecessary variables\n",
        "data_with_targets = data_preprocessed.drop(['Absenteeism Time in Hours'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUBHoHCoPU3S"
      },
      "source": [
        "# check if the line above is a checkpoint :)\n",
        "\n",
        "# if data_with_targets is data_preprocessed = True, then the two are pointing to the same object\n",
        "# if it is False, then the two variables are completely different and this is in fact a checkpoint\n",
        "data_with_targets is data_preprocessed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbgnUCHkPU3Y"
      },
      "source": [
        "# check what's inside\n",
        "data_with_targets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WHKQVXBPU3c"
      },
      "source": [
        "## Select the inputs for the regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p52A3GF-PU3c"
      },
      "source": [
        "data_with_targets.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "012AIi-hPU3j"
      },
      "source": [
        "# selects all rows and all columns until 14 (excluding)\n",
        "data_with_targets.iloc[:,:14]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VrbAeDKCPU3q"
      },
      "source": [
        "# selects all rows and all columns but the last one (basically the same operation)\n",
        "data_with_targets.iloc[:,:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjeTpTioPU3v"
      },
      "source": [
        "# create a variable that will contain the inputs (everything without the targets)\n",
        "unscaled_inputs = data_with_targets.iloc[:,:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxxi2bOuPU3z"
      },
      "source": [
        "## Standardize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56b-pYFPU30"
      },
      "source": [
        "# standardize the inputs\n",
        "\n",
        "# standardization is one of the most common preprocessing tools\n",
        "# since data of different magnitude (scale) can be biased towards high values,\n",
        "# we want all inputs to be of similar magnitude\n",
        "# this is a peculiarity of machine learning in general - most (but not all) algorithms do badly with unscaled data\n",
        "\n",
        "# a very useful module we can use is StandardScaler \n",
        "# it has much more capabilities than the straightforward 'preprocessing' method\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# we will create a variable that will contain the scaling information for this particular dataset\n",
        "# here's the full documentation: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "\n",
        "# define scaler as an object\n",
        "absenteeism_scaler = StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjbGKxI8PU33"
      },
      "source": [
        "# fit the unscaled_inputs\n",
        "# this basically calculates the mean and standard deviation of each feature\n",
        "absenteeism_scaler.fit(unscaled_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF2D-SH2PU36"
      },
      "source": [
        "# transform the unscaled inputs\n",
        "# this is the scaling itself - we subtract the mean and divide by the standard deviation\n",
        "scaled_inputs = absenteeism_scaler.transform(unscaled_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPpy1TT3PU38"
      },
      "source": [
        "# eyeball the newly created variable\n",
        "scaled_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31657KD9PU3-"
      },
      "source": [
        "# check the shape of the array\n",
        "scaled_inputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-fzPoIQPU4B"
      },
      "source": [
        "## Split the data into train & test and shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD_Q2Md6PU4B"
      },
      "source": [
        "### Import the relevant module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztSPzt_4PU4C"
      },
      "source": [
        "# import train_test_split so we can split our data into train and test\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bj4Z1QzPU4E"
      },
      "source": [
        "### Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-mcLHqvDPU4F"
      },
      "source": [
        "# check how this method works\n",
        "train_test_split(scaled_inputs, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH7LYonSPU4K"
      },
      "source": [
        "# declare 4 variables for the split\n",
        "x_train, x_test, y_train, y_test = train_test_split(scaled_inputs, targets, #train_size = 0.8, \n",
        "                                                                            test_size = 0.2, random_state = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCJEQY5wPU4O"
      },
      "source": [
        "# check the shape of the train inputs and targets\n",
        "print (x_train.shape, y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDVA3oD9PU4S"
      },
      "source": [
        "# check the shape of the test inputs and targets\n",
        "print (x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnjbuH0_PU4V"
      },
      "source": [
        "## Logistic regression with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC967VJsPU4V"
      },
      "source": [
        "# import the LogReg model from sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# import the 'metrics' module, which includes important metrics we may want to use\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O1Ol0T3PU4Z"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuBsjthXPU4a"
      },
      "source": [
        "# create a logistic regression object\n",
        "reg = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leLp8wDIPU4e"
      },
      "source": [
        "# fit our train inputs\n",
        "# that is basically the whole training part of the machine learning\n",
        "reg.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdgJhpzkPU4g"
      },
      "source": [
        "# assess the train accuracy of the model\n",
        "reg.score(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVQKG4U4PU4k"
      },
      "source": [
        "### Manually check the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EJazJ11fPU4k"
      },
      "source": [
        "# find the model outputs according to our model\n",
        "model_outputs = reg.predict(x_train)\n",
        "model_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPxBMScfPU4o"
      },
      "source": [
        "# compare them with the targets\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AHuJwrsiPU4t"
      },
      "source": [
        "# ACTUALLY compare the two variables\n",
        "model_outputs == y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izLt_t5lPU4w"
      },
      "source": [
        "# find out in how many instances we predicted correctly\n",
        "np.sum((model_outputs==y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVW4CR0SPU4z"
      },
      "source": [
        "# get the total number of instances\n",
        "model_outputs.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teZCKV8yPU42"
      },
      "source": [
        "# calculate the accuracy of the model\n",
        "# divide the number of correctly predicted outputs by the number of all outputs\n",
        "np.sum((model_outputs==y_train)) / model_outputs.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxf1996EPU45"
      },
      "source": [
        "### Finding the intercept and coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DRJARTiPU47"
      },
      "source": [
        "# get the intercept (bias) of our model\n",
        "reg.intercept_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IDTzHZ5PU4_"
      },
      "source": [
        "# get the coefficients (weights) of our model\n",
        "reg.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbZBUMzrPU5C"
      },
      "source": [
        "# check what were the names of our columns\n",
        "unscaled_inputs.columns.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U77kC8FdPU5F"
      },
      "source": [
        "# save the names of the columns in an ad-hoc variable\n",
        "feature_name = unscaled_inputs.columns.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABM5OtUKPU5I"
      },
      "source": [
        "# use the coefficients from this table (they will be exported later and will be used in Tableau)\n",
        "# transpose the model coefficients (model.coef_) and throws them into a df (a vertical organization, so that they can be\n",
        "# multiplied by certain matrices later) \n",
        "summary_table = pd.DataFrame (columns=['Feature name'], data = feature_name)\n",
        "\n",
        "# add the coefficient values to the summary table\n",
        "summary_table['Coefficient'] = np.transpose(reg.coef_)\n",
        "\n",
        "# display the summary table\n",
        "summary_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ywteATTrPU5M"
      },
      "source": [
        "# do a little Python trick to move the intercept to the top of the summary table\n",
        "# move all indices by 1\n",
        "summary_table.index = summary_table.index + 1\n",
        "\n",
        "# add the intercept at index 0\n",
        "summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n",
        "\n",
        "# sort the df by index\n",
        "summary_table = summary_table.sort_index()\n",
        "summary_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQkfwd9mPU5R"
      },
      "source": [
        "## Interpreting the coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsgbsOMzPU5S"
      },
      "source": [
        "# create a new Series called: 'Odds ratio' which will show the.. odds ratio of each feature\n",
        "summary_table['Odds_ratio'] = np.exp(summary_table.Coefficient)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT3ABY9KPU5V"
      },
      "source": [
        "# display the df\n",
        "summary_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeh2rD17PU5Y"
      },
      "source": [
        "# sort the table according to odds ratio\n",
        "# note that by default, the sort_values method sorts values by 'ascending'\n",
        "summary_table.sort_values('Odds_ratio', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}